{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection - simplistic greedy approach\n",
    "As we've now seen, it's fairly easy to overfit a model and as such we may need to make decisions about what variables or factors to include in the model and which to leave out. A simplistic way to do this is to add features individually, one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split the data into a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53617\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>CAT_Insurer</th>\n",
       "      <th>CAT_Region_Num</th>\n",
       "      <th>205d_V2_CUR</th>\n",
       "      <th>205d_V3_PRC</th>\n",
       "      <th>212d_V1_CUR</th>\n",
       "      <th>212d_V2_PRC</th>\n",
       "      <th>213d_V2_CUR</th>\n",
       "      <th>213d_V3_PRC</th>\n",
       "      <th>...</th>\n",
       "      <th>KG_SPS_226d_V1_CUR</th>\n",
       "      <th>KG_SPS_227d_V1_PRC</th>\n",
       "      <th>KG_SPS_229d_V1_CUR</th>\n",
       "      <th>KG_SX_226d_V1_CUR</th>\n",
       "      <th>KG_SX_227d_V1_PRC</th>\n",
       "      <th>KG_SX_229d_V1_CUR</th>\n",
       "      <th>KG_TOT_226d_V1_CUR</th>\n",
       "      <th>KG_TOT_227d_V1_PRC</th>\n",
       "      <th>KG_TOT_229d_V1_CUR</th>\n",
       "      <th>Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.409432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.394941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.358463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.321986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.234051</td>\n",
       "      <td>0.377469</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275441</td>\n",
       "      <td>0.373868</td>\n",
       "      <td>0.285167</td>\n",
       "      <td>0.149472</td>\n",
       "      <td>0.610291</td>\n",
       "      <td>0.131174</td>\n",
       "      <td>0.326051</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.359147</td>\n",
       "      <td>0.285634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID  CAT_Insurer  CAT_Region_Num  205d_V2_CUR  \\\n",
       "0           0  0.000000          0.0             0.0     0.326051   \n",
       "1           1  0.000019          0.0             0.0     0.326051   \n",
       "2           2  0.000037          0.0             0.0     0.326051   \n",
       "3           3  0.000056          0.0             0.0     0.326051   \n",
       "4           4  0.000075          0.0             0.0     0.326051   \n",
       "\n",
       "   205d_V3_PRC  212d_V1_CUR  212d_V2_PRC  213d_V2_CUR  213d_V3_PRC    ...     \\\n",
       "0     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "1     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "2     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "3     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "4     0.215957     0.322181     0.234051     0.377469     0.213745    ...      \n",
       "\n",
       "   KG_SPS_226d_V1_CUR  KG_SPS_227d_V1_PRC  KG_SPS_229d_V1_CUR  \\\n",
       "0            0.275441            0.373868            0.285167   \n",
       "1            0.275441            0.373868            0.285167   \n",
       "2            0.275441            0.373868            0.285167   \n",
       "3            0.275441            0.373868            0.285167   \n",
       "4            0.275441            0.373868            0.285167   \n",
       "\n",
       "   KG_SX_226d_V1_CUR  KG_SX_227d_V1_PRC  KG_SX_229d_V1_CUR  \\\n",
       "0           0.149472           0.610291           0.131174   \n",
       "1           0.149472           0.610291           0.131174   \n",
       "2           0.149472           0.610291           0.131174   \n",
       "3           0.149472           0.610291           0.131174   \n",
       "4           0.149472           0.610291           0.131174   \n",
       "\n",
       "   KG_TOT_226d_V1_CUR  KG_TOT_227d_V1_PRC  KG_TOT_229d_V1_CUR   Premium  \n",
       "0            0.326051            0.215957            0.359147  0.409432  \n",
       "1            0.326051            0.215957            0.359147  0.394941  \n",
       "2            0.326051            0.215957            0.359147  0.358463  \n",
       "3            0.326051            0.215957            0.359147  0.321986  \n",
       "4            0.326051            0.215957            0.359147  0.285634  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Swiss_Healthcare_Premium_Prediction.csv.gz', compression='gzip')\n",
    "\n",
    "df = df.fillna(value=0)\n",
    "X = df[df.columns[:-1]]\n",
    "y = df['Premium']\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find the [single] best feature to train a regression model on\n",
    "Loop through all of the X features and train an unpenalized LinearRegression model using each of those single features. Find the feature that produces the lowest Mean squared test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The single best predictor was: KG_TOT_229d_V1_CUR\n"
     ]
    }
   ],
   "source": [
    "min_mse = 100**100 #super high number to guarantee it wont be the min\n",
    "best_feature = None\n",
    "\n",
    "def mse(residual_col):\n",
    "    return np.mean(residual_col.map(lambda x: x**2))\n",
    "\n",
    "for feat in X.columns:\n",
    "    linreg = LinearRegression()\n",
    "    curr_X_train = np.array(X_train[feat]).reshape(-1, 1)\n",
    "    curr_X_test = np.array(X_test[feat]).reshape(-1,1)\n",
    "    linreg.fit(curr_X_train,y_train)\n",
    "    \n",
    "    y_hat_test = linreg.predict(curr_X_test)\n",
    "    test_mse = mse(y_test - y_hat_test)\n",
    "    \n",
    "    if test_mse < min_mse:\n",
    "        min_mse = test_mse\n",
    "        best_feature = feat\n",
    "        \n",
    "print('The single best predictor was: {}'.format(feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generalize #2\n",
    "Write a function that takes in a desired number of features and returns a model using the top n features (according to test set error). Be sure to do this iteratively. In other words, rather then simply taking the top n features based on how well each performs individually, first find the best feature and train a model, then loop back through all of the remaining features and select that which produces the best results in combination with the best feature already selected. Continue on finding the best third feature in combination with the previous 2 features, etc. This process will continue until you reach the desired number of features (or there are no features left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feat(X_train, X_test, y_train, y_test, feat_options, prev_feats=[]):\n",
    "    min_mse = 100**100\n",
    "    best_feature = None\n",
    "    \n",
    "    for feat in feat_options:\n",
    "        linreg = LinearRegression()\n",
    "        if prev_feats==[]:\n",
    "            curr_X_train = np.array(X_train[feat]).reshape(-1, 1)\n",
    "            curr_X_test = np.array(X_test[feat]).reshape(-1,1)\n",
    "        else:\n",
    "            feats = prev_feats + [feat]\n",
    "            curr_X_train = X_train[feats]\n",
    "            curr_X_test = X_test[feats]\n",
    "            \n",
    "        linreg.fit(curr_X_train,y_train)\n",
    "        \n",
    "        y_hat_test = linreg.predict(curr_X_test)\n",
    "        test_mse = mse(y_test - y_hat_test)\n",
    "        \n",
    "        if test_mse < min_mse:\n",
    "            min_mse = test_mse\n",
    "            best_feature = feat\n",
    "    return best_feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_greedy_feat(n_feats, X_train, X_test, y_train, y_test):\n",
    "    curr_model_feats = []\n",
    "    remaining_feats = list(X.columns)\n",
    "    \n",
    "    for n in range(1,n_feats+1):\n",
    "        next_feat = best_feat(X_train, X_test, y_train, y_test,\n",
    "                              feat_options=remaining_feats, prev_feats = curr_model_feats)\n",
    "        curr_model_feats.append(next_feat)\n",
    "        remaining_feats.remove(next_feat)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train[curr_model_feats], y_train)\n",
    "    \n",
    "    return model, curr_model_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'213d_V1_CUR'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_feat(X_train,X_test,y_train,y_test,feat_options=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " ['213d_V1_CUR',\n",
       "  '212d_V1B_CUR',\n",
       "  '212d_V1_CUR',\n",
       "  '224d_V1_CUR',\n",
       "  '205d_V2_CUR',\n",
       "  '716d_V1_INT',\n",
       "  '501d_V11_CUR',\n",
       "  '215d_V1_CUR_ICEP',\n",
       "  '708d_V1_INT',\n",
       "  'CAT_Region_Num'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_greedy_feat(10, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Learning Curves\n",
    "Iterate from 2 to 20 feature variables. Use your greedy classifier defined above to generate a linear regression model with successively more and more features incorporated into the model. Then plot the train and test errors as a function of the number of variables incorporated into each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(2,21):\n",
    "    print('On iteration: {}'.format(i-1))\n",
    "    #Train Greedy Classifier Model with this many features\n",
    "    #Your code here\n",
    "    \n",
    "    #Calculate Training Mean Squared Error\n",
    "    #Your code here\n",
    "    \n",
    "    #Calculate Test Mean Squared Error\n",
    "    #Your code here\n",
    "    #Plot Results\n",
    "    #Your code here\n",
    "    pass\n",
    "#Add Legend and Descriptive Title/Axis Labels\n",
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
